syntax = "proto3";

package doublecloud.transfer.v1;

option go_package = "github.com/doublecloud/go-genproto/doublecloud/transfer/v1;transfer";

import "doublecloud/transfer/v1/endpoint/common.proto";
import "doublecloud/transfer/v1/endpoint.proto";

enum TransferType {
    TRANSFER_TYPE_UNSPECIFIED = 0;
    // Snapshot and increment
    SNAPSHOT_AND_INCREMENT = 1;
    // Snapshot
    SNAPSHOT_ONLY = 2;
    // Increment
    INCREMENT_ONLY = 3;
}
enum TransferStatus {
    TRANSFER_STATUS_UNSPECIFIED = 0;
    // Transfer does some work before running
    CREATING = 1;
    // Transfer created but not started by user
    CREATED = 2;
    // Transfer currently doing replication work
    RUNNING = 3;
    // Transfer shutdown
    STOPPING = 4;
    // Transfer stopped by user
    STOPPED = 5;
    // Transfer stopped by system
    ERROR = 6;
    // Transfer copy snapshot
    SNAPSHOTTING = 7;
    // Transfer reach terminal phase
    DONE = 8;
}
enum RegularSnapshotScheduleInterval {
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_UNSPECIFIED = 0;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_15MIN = 2;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_30MIN = 3;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_HOUR = 4;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_2HOUR = 5;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_3HOUR = 6;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_6HOUR = 7;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_8HOUR = 8;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_12HOUR = 9;
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_DAY = 10;
}
enum Flavor {
    FLAVOR_UNSPECIFIED = 0;
    SMALL = 1;
    MEDIUM = 2;
    LARGE = 3;
    TINY = 4;
}
// Transfer core entity
message Transfer {
    reserved 3, 11, 13 to 14, 18;
    string id = 1;
    string project_id = 2;
    string name = 4;
    string description = 5;
    map<string,string> labels = 6;
    Endpoint source = 7;
    Endpoint target = 8;
    Runtime runtime = 9;
    TransferStatus status = 10;
    TransferType type = 12;
    string warning = 15;
    RegularSnapshot regular_snapshot = 16;
    Transformation transformation = 17;
    DataObjects data_objects = 19;
}
message Runtime {
    reserved 1 to 4;
    oneof runtime {
        ServerlessRuntime serverless_runtime = 5;
        DedicatedRuntime dedicated_runtime = 6;
    }
}
message ServerlessRuntime {
    reserved 1;
    int64 job_count = 2;
}
message RegularSnapshot {
    reserved 1 to 2;
    oneof mode {
        RegularSnapshotSettings settings = 3;
        RegularSnapshotDisabled disabled = 4;
    }
}
message RegularSnapshotDisabled {
}
message RegularSnapshotSettings {
    RegularSnapshotScheduleInterval schedule = 1;
    repeated IncrementalTable tables = 2;
    string cron_expression = 3;
}
message IncrementalTable {
    string table_namespace = 1;
    string table_name = 2;
    string cursor_column = 3;
    string initial_state = 4;
}
message DedicatedRuntime {
    reserved 1, 3 to 7;
    Flavor flavor = 2;
    Settings settings = 8;
}
message Settings {
    oneof settings {
        AutoSettings auto_settings = 1;
        ManualSettings manual_settings = 2;
    }
}
message AutoSettings {
}
message ManualSettings {
    reserved 1 to 3;
    string network_id = 4;
}
// Mask function
message MaskFunction {
    oneof mask_function {
        // Hash mask function
        MaskFunctionHash mask_function_hash = 1;
    }
}
// Hash data using HMAC
message MaskFunctionHash {
    // This string will be used in the HMAC(sha256, salt) function applied to the
    // column data.
    string user_defined_salt = 1;
}
// Filter tables using lists of included and excluded tables.
message TablesFilter {
    // List of tables that will be included to transfer
    repeated string include_tables = 1;
    // List of tables that will be excluded to transfer
    repeated string exclude_tables = 2;
}
// Filter columns using lists of included and excluded columns.
message ColumnsFilter {
    // List of columns that will be included to transfer
    repeated string include_columns = 1;
    // List of columns that will be excluded to transfer
    repeated string exclude_columns = 2;
}
// Mask field transformer allows you to hash data
message MaskFieldTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // Specify the name of the column for data masking (a regular expression).
    repeated string columns = 2;
    // Mask function
    MaskFunction function = 3;
}
// Set up a list of table columns to transfer
message FilterColumnsTransformer {
    // List of the tables to filter using lists of included and excluded tables.
    TablesFilter tables = 1;
    // List of the columns to transfer to the target tables using lists of included and
    // excluded columns.
    ColumnsFilter columns = 2;
}
message Table {
    string name_space = 1;
    string name = 2;
}
// Specify rule for renaming table
message RenameTable {
    // Specify the current names of the table in the source
    Table original_name = 1;
    // Specify the new names for this table in the target
    Table new_name = 2;
}
// Set rules for renaming tables by specifying the current names of the tables in
// the source and new names for these tables in the target.
message RenameTablesTransformer {
    // List of renaming rules
    repeated RenameTable rename_tables = 1;
}
message SkipEventsTransformer {
    TablesFilter tables = 1;
}
// Override primary keys
message ReplacePrimaryKeyTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // List of columns to be used as primary keys
    repeated string keys = 2;
}
// Convert column values to strings
// The values will be converted depending on the source type
// Conversion rules are described here:
// https://cloud.yandex.com/en/docs/data-transfer/concepts/data-transformation#convert-to-string
message ToStringTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // List of included and excluded columns
    ColumnsFilter columns = 2;
}
// Set the number of shards for particular tables and a list of columns whose
// values will be used for calculating a hash to determine a shard.
message SharderTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // List of included and excluded columns
    ColumnsFilter columns = 2;
    // Number of shards
    int64 shards_count = 3;
}
message RawDocGroupTransformer {
    TablesFilter tables = 1;
    repeated string keys = 2;
    repeated string fields = 3;
}
message RawCdcDocGroupTransformer {
    reserved 1;
    repeated string keys = 2;
    repeated string fields = 3;
}
message SQLTransformer {
    TablesFilter tables = 1;
    string query = 2;
}
message DBTTransformer {
    enum Operation {
        OPERATION_UNSPECIFIED = 0;
        OPERATION_BUILD = 1;
        OPERATION_COMPILE = 2;
        OPERATION_DEBUG = 3;
        OPERATION_PARSE = 4;
        OPERATION_RUN = 5;
        OPERATION_SEED = 6;
        OPERATION_SNAPSHOT = 7;
        OPERATION_TEST = 8;
    }
    string git_repository_link = 1;
    string git_branch = 2;
    string profile_name = 3;
    Operation operation = 4;
}
// A transfer splits the X table into multiple tables (X_1, X_2, ..., X_n) based on
// data.
// If a row was located in the X table before it was split, it is now in the X_i
// table,
// where i is determined by the column list and split string parameters.
// Example:
// If the column list has two columns, month of birth and gender, specified and the
// split string states @,
// information about an employee whose name is John and who was born on February
// 11, 1984,
// from the Employees table will get to a new table named Employees@February@male.
message TableSplitterTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // Specify the columns in the tables to be partitioned.
    repeated string columns = 2;
    // Specify the split string to be used for merging components in a new table name.
    string splitter = 3;
}
// This filter only applies to transfers with queues (Logbroker or Apache KafkaÂ®)
// as a data source.
// When running a transfer, only the strings meeting the specified criteria remain
// in a changefeed.
message FilterRowsTransformer {
    // List of included and excluded tables
    TablesFilter tables = 1;
    // Filtering criterion. This can be comparison operators for numeric, string, and
    // Boolean values,
    // comparison to NULL, and checking whether a substring is part of a string.
    // Details here:
    // https://cloud.yandex.com/en/docs/data-transfer/concepts/data-transformation#append-only-sources
    string filter = 2;
}
message NumberToFloatTransformer {
    TablesFilter tables = 1;
}
// Some transformers may have limitations and only apply to some source-target
// pairs.
message Transformer {
    reserved 5, 8, 10, 16 to 17;
    oneof transformer {
        MaskFieldTransformer mask_field = 1;
        FilterColumnsTransformer filter_columns = 2;
        SkipEventsTransformer skip_events = 3;
        RenameTablesTransformer rename_tables = 4;
        ReplacePrimaryKeyTransformer replace_primary_key = 6;
        ToStringTransformer convert_to_string = 7;
        SharderTransformer sharder_transformer = 9;
        SQLTransformer sql = 11;
        DBTTransformer dbt = 12;
        TableSplitterTransformer table_splitter_transformer = 13;
        FilterRowsTransformer filter_rows = 14;
        NumberToFloatTransformer number_to_float_transformer = 15;
        CloudFunctionTransformer cloud_function_transformer = 18;
    }
}
message CloudFunctionTransformer {
    string name = 1;
    string name_space = 2;
    endpoint.DataTransformationOptions options = 3;
}
// Transformation is converting data using special transformer functions.
// These functions are executed on a data stream, applied to each data change item,
// and transform them.
// A transformer can be run at both the metadata and data levels.
// Data can only be transformed if the source and target are of different types.
message Transformation {
    // Transformers are set as a list.
    // When activating a transfer, a transformation plan is made for the tables that
    // match the specified criteria.
    // Transformers are applied to the tables in the sequence specified in the list.
    repeated Transformer transformers = 1;
}
message DataObjects {
    repeated string include_objects = 1;
}
